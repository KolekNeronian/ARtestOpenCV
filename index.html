<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Custom OpenCV.js AR Engine</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: monospace;}
        
        #videoInput { a
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            object-fit: cover; z-index: 1;
        }
        canvas { 
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            z-index: 2; pointer-events: none; 
        }

        #ui-layer {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 10;
            pointer-events: none; display: flex; flex-direction: column; justify-content: space-between;
        }
        
        #status-bar {
            background: rgba(0, 0, 0, 0.7); color: #00ff00; padding: 15px;
            text-align: center; font-size: 16px; border-bottom: 1px solid #00ff00;
        }

        #debug-log {
            background: rgba(0, 0, 0, 0.5); color: #fff; padding: 10px;
            font-size: 10px; text-align: left; max-height: 100px; overflow-y: auto;
            pointer-events: auto;
        }

        #instructions {
            padding: 20px; color: white; text-align: center; font-size: 18px; 
            font-weight: bold; text-shadow: 0px 2px 4px black;
            background: linear-gradient(to top, rgba(0,0,0,0.8), transparent);
        }
    </style>
</head>
<body>

    <div id="ui-layer">
        <div id="status-bar">Initializing System...</div>
        <div>
            <div id="instructions">SCANNING... WAIT FOR DOTS THEN TAP</div>
            <div id="debug-log">Log started...</div>
        </div>
    </div>

    <video id="videoInput" playsinline webkit-playsinline></video>
    <canvas id="canvasOutput"></canvas>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js" crossorigin="anonymous"></script>

    <script>
        function log(msg) {
            const logDiv = document.getElementById('debug-log');
            logDiv.innerHTML += `<div>> ${msg}</div>`;
            logDiv.scrollTop = logDiv.scrollHeight;
            console.log(msg);
        }

        function updateStatus(msg) {
            document.getElementById('status-bar').innerText = msg;
        }

        var Module = {
            onRuntimeInitialized: function() {
                updateStatus("OpenCV Compiled.");
                log("OpenCV WASM Ready.");
                startCamera();
            }
        };
    </script>

    <script async src="https://docs.opencv.org/4.8.0/opencv.js" type="text/javascript"></script>

    <script>
        let video, canvas;
        let renderer, scene, camera, cube, featurePointsMesh;
        let isTracking = false;
        let isScanning = false;
        let streamWidth, streamHeight;

        let cap, oldFrame, oldGray, frame, gray;
        let p0, p1, st, err;
        let initialObjectPoints = [];
        let rvec, tvec, distCoeffs, cameraMatrix;
        let useFallbackMath = false;

        // PnP Variables
        let imagePointsMat, objectPointsMat;

        async function startCamera() {
            video = document.getElementById('videoInput');
            updateStatus("Requesting Camera...");

            const constraints = {
                video: {
                    facingMode: 'environment',
                    width: { ideal: 640 },
                    height: { ideal: 480 }
                },
                audio: false
            };

            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.muted = true;
                
                const playPromise = video.play();
                if (playPromise !== undefined) {
                    playPromise.catch(error => {
                        updateStatus("TAP SCREEN TO ENABLE CAMERA");
                        document.body.addEventListener('click', () => video.play(), { once: true });
                    });
                }

                const checkVideo = setInterval(() => {
                    if (video.videoWidth > 0 && video.videoHeight > 0) {
                        clearInterval(checkVideo);
                        streamWidth = video.videoWidth;
                        streamHeight = video.videoHeight;
                        video.width = streamWidth;
                        video.height = streamHeight;
                        log(`Video Ready: ${streamWidth}x${streamHeight}`);
                        initAR();
                    }
                }, 100);

            } catch (err) {
                updateStatus("Camera Error");
                log("Error: " + err.message);
            }
        }

        function initAR() {
            try {
                log("Initializing AR Engine...");
                
                if (typeof cv.solvePnP !== 'function') {
                    log("WARNING: cv.solvePnP missing. Using Homography fallback.");
                    useFallbackMath = true;
                }

                canvas = document.getElementById('canvasOutput');
                canvas.width = window.innerWidth;
                canvas.height = window.innerHeight;
                
                initThreeJS();

                cap = new cv.VideoCapture(video);
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                
                p0 = new cv.Mat();
                p1 = new cv.Mat();
                st = new cv.Mat();
                err = new cv.Mat();

                const focalLength = streamWidth; 
                const centerX = streamWidth / 2;
                const centerY = streamHeight / 2;
                
                cameraMatrix = cv.matFromArray(3, 3, cv.CV_64FC1, [
                    focalLength, 0, centerX,
                    0, focalLength, centerY,
                    0, 0, 1
                ]);
                
                distCoeffs = cv.Mat.zeros(4, 1, cv.CV_64FC1);
                rvec = new cv.Mat();
                tvec = new cv.Mat();

                updateStatus("SCANNING SURFACES...");
                log("Scan Mode Active.");
                
                // Start Scanning immediately
                isScanning = true;
                document.body.addEventListener('click', handleUserClick);
                requestAnimationFrame(processLoop);

            } catch (e) {
                log("Init Crashed: " + e.message);
            }
        }

        function initThreeJS() {
            scene = new THREE.Scene();
            // Create a camera that matches video aspect ratio but renders to full screen
            const fov = 2 * Math.atan((streamHeight / 2) / streamWidth) * (180 / Math.PI);
            camera = new THREE.PerspectiveCamera(fov, window.innerWidth / window.innerHeight, 0.1, 1000);
            
            // Create a dedicated "Screen Camera" for 2D overlays (Points)
            // This allows us to draw points in 2D screen space (X,Y) easily
            
            renderer = new THREE.WebGLRenderer({ canvas: canvas, alpha: true, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            scene.add(new THREE.AmbientLight(0xffffff, 1));
            const dl = new THREE.DirectionalLight(0xffffff, 0.8);
            dl.position.set(0, 1, 1);
            scene.add(dl);

            scene.add(new THREE.AxesHelper(1));

            const geometry = new THREE.BoxGeometry(0.5, 0.5, 0.5);
            const material = new THREE.MeshNormalMaterial({ wireframe: false }); 
            cube = new THREE.Mesh(geometry, material);
            cube.visible = false; 
            scene.add(cube);

            // Create Point Cloud for Scanning Visualization
            const pointsGeo = new THREE.BufferGeometry();
            // Max 100 points
            const positions = new Float32Array(100 * 3); 
            pointsGeo.setAttribute('position', new THREE.BufferAttribute(positions, 3));
            const pointsMat = new THREE.PointsMaterial({ color: 0xffff00, size: 5, sizeAttenuation: false });
            featurePointsMesh = new THREE.Points(pointsGeo, pointsMat);
            // We want these points to be drawn in screen space effectively
            // But to simplify, we will put them in the 3D scene at a fixed depth during scanning
            featurePointsMesh.frustumCulled = false;
            scene.add(featurePointsMesh);
        }

        function handleUserClick(e) {
            if (isTracking) {
                // Reset to scanning
                isTracking = false;
                isScanning = true;
                cube.visible = false;
                featurePointsMesh.visible = true;
                updateStatus("SCANNING... Tap to Place");
                return;
            }

            if (isScanning && p0.rows > 10) {
                // LOCK IN
                isScanning = false;
                startTracking();
            } else {
                updateStatus("Not enough points found yet!");
            }
        }

        function checkDimensions() {
            if (video.videoWidth !== streamWidth || video.videoHeight !== streamHeight) {
                streamWidth = video.videoWidth;
                streamHeight = video.videoHeight;
                video.width = streamWidth;
                video.height = streamHeight;
                
                frame.delete(); gray.delete(); oldFrame.delete(); oldGray.delete();
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                return true; 
            }
            return false;
        }

        function updateScanVisuals(pointMat) {
            // Update the Three.js Point Cloud positions based on OpenCV detected points
            const positions = featurePointsMesh.geometry.attributes.position.array;
            let ptr = 0;
            
            // We need to map 2D video pixel coordinates to 3D world coordinates 
            // that sit in front of the camera so the user can see them "overlaying" the video
            
            // Simple approach: Unproject from screen to Z=-10
            // Since our camera is static during scan (we assume 0,0,0 position for scan visualization)
            
            // Actually, simplest way: Just map X/Y pixels to X/Y world units at fixed Z
            // Scale factor:
            const scale = 0.01;
            
            for (let i = 0; i < pointMat.rows; i++) {
                let x = pointMat.data32F[i * 2];
                let y = pointMat.data32F[i * 2 + 1];
                
                // Center and scale
                positions[ptr++] = (x - streamWidth/2) * scale;
                positions[ptr++] = -(y - streamHeight/2) * scale;
                positions[ptr++] = -5; // Fixed depth for visualization
            }
            
            // Zero out remaining points (hide them)
            for (let i = pointMat.rows; i < 100; i++) {
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
            }
            
            featurePointsMesh.geometry.attributes.position.needsUpdate = true;
            
            // During scan, camera must be at origin looking forward
            camera.position.set(0,0,0);
            camera.rotation.set(0,0,0);
        }

        function startTracking() {
            log("Locking features...");
            
            // The points currently in 'p0' (from the last scan frame) become our Anchor World Points
            initialObjectPoints = [];
            for (let i = 0; i < p0.rows; i++) {
                let x = p0.data32F[i * 2];
                let y = p0.data32F[i * 2 + 1];
                
                // Map these 2D points to 3D Z=0 plane
                // This "defines" the surface we just clicked on
                initialObjectPoints.push((x - streamWidth/2) * 0.01);
                initialObjectPoints.push(-(y - streamHeight/2) * 0.01); 
                initialObjectPoints.push(0); 
            }

            // We need to keep 'oldGray' populated for Optical Flow to have a reference
            // It should already be populated from the scan loop
            
            isTracking = true;
            cube.visible = true;
            featurePointsMesh.visible = false; // Hide the dots
            
            // Place cube at center of found points? 
            // For now, place at 0,0,0 (center of screen at tap moment)
            cube.position.set(0,0,0);
            
            updateStatus("LOCKED! Move Camera.");
        }

        function processLoop() {
            if (!video || !video.videoWidth) {
                requestAnimationFrame(processLoop);
                return;
            }

            try {
                // Always read new frame
                cap.read(frame);
                cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

                if (isScanning) {
                    // === SCAN MODE ===
                    // Constantly find good features
                    let none = new cv.Mat();
                    cv.goodFeaturesToTrack(gray, p0, 100, 0.1, 7, none, 7, false, 0.04);
                    
                    updateScanVisuals(p0);
                    
                    // Keep this frame as 'oldGray' for the moment we switch to tracking
                    gray.copyTo(oldGray);
                    none.delete();
                    
                } else if (isTracking) {
                    // === TRACK MODE ===
                    // Optical Flow from Locked Frame -> Current Frame
                    cv.calcOpticalFlowPyrLK(oldGray, gray, p0, p1, st, err, new cv.Size(21, 21), 2);

                    let goodNew = [];
                    let goodOld3D = [];
                    let validCount = 0;
                    
                    for (let i = 0; i < st.rows; i++) {
                        if (st.data[i] === 1) {
                            goodNew.push(p1.data32F[i*2]);
                            goodNew.push(p1.data32F[i*2+1]);
                            
                            goodOld3D.push(initialObjectPoints[i*3]);
                            goodOld3D.push(initialObjectPoints[i*3+1]);
                            goodOld3D.push(initialObjectPoints[i*3+2]);
                            validCount++;
                        }
                    }

                    if (validCount >= 4) {
                        if(imagePointsMat) imagePointsMat.delete();
                        if(objectPointsMat) objectPointsMat.delete();
                        
                        imagePointsMat = cv.matFromArray(validCount, 2, cv.CV_64FC1, goodNew);
                        
                        if (useFallbackMath) {
                            solveHomographyPose(goodOld3D, goodNew, rvec, tvec);
                        } else {
                            objectPointsMat = cv.matFromArray(validCount, 3, cv.CV_64FC1, goodOld3D);
                            const success = cv.solvePnP(objectPointsMat, imagePointsMat, cameraMatrix, distCoeffs, rvec, tvec, false, cv.SOLVEPNP_ITERATIVE);
                            if (success) updateThreeJSCamera(rvec, tvec);
                        }
                    } else {
                        isTracking = false;
                        isScanning = true;
                        cube.visible = false;
                        featurePointsMesh.visible = true;
                        updateStatus("Lost Track. Scanning...");
                    }
                }

            } catch(e) {
                log("Loop Error: " + e.message);
                isTracking = false;
                isScanning = true;
            }
            
            renderer.render(scene, camera);
            requestAnimationFrame(processLoop);
        }

        function solveHomographyPose(objPoints, imgPoints, rvec, tvec) {
            // Basic translation fallback
            let cx_new = 0, cy_new = 0;
            let cx_old = 0, cy_old = 0;
            const count = imgPoints.length / 2;
            
            for(let i=0; i<count; i++) {
                cx_new += imgPoints[i*2];
                cy_new += imgPoints[i*2+1];
                cx_old += (objPoints[i*3] / 0.01) + streamWidth/2;
                cy_old += -(objPoints[i*3+1] / 0.01) + streamHeight/2;
            }
            
            const dx = ((cx_new/count) - (cx_old/count));
            const dy = ((cy_new/count) - (cy_old/count));
            
            tvec.data64F[0] = dx * 0.01;
            tvec.data64F[1] = -dy * 0.01;
            tvec.data64F[2] = 0;
            
            rvec.data64F[0] = 0; rvec.data64F[1] = 0; rvec.data64F[2] = 0;
            updateThreeJSCamera(rvec, tvec);
        }

        function updateThreeJSCamera(rvec, tvec) {
            try {
                const R = new cv.Mat();
                cv.Rodrigues(rvec, R);

                const t = [tvec.data64F[0], tvec.data64F[1], tvec.data64F[2]];
                const r = R.data64F;

                const m = new THREE.Matrix4();
                m.set(
                    r[0], r[1], r[2], t[0],
                    r[3], r[4], r[5], t[1],
                    r[6], r[7], r[8], t[2],
                    0, 0, 0, 1
                );

                const inverse = m.invert();
                camera.quaternion.setFromRotationMatrix(inverse);
                camera.position.setFromMatrixPosition(inverse);
                camera.rotateX(Math.PI); 

                R.delete();
            } catch(e) { }
        }
    </script>
</body>
</html>