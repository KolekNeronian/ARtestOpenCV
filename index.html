<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Custom OpenCV.js AR Engine</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: monospace;}
        
        #videoInput { 
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            object-fit: cover; z-index: 1;
        }
        canvas { 
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            z-index: 2; pointer-events: none; 
        }

        #ui-layer {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 10;
            pointer-events: none; display: flex; flex-direction: column; justify-content: space-between;
        }
        
        #status-bar {
            background: rgba(0, 0, 0, 0.7); color: #00ff00; padding: 15px;
            text-align: center; font-size: 16px; border-bottom: 1px solid #00ff00;
        }

        #debug-log {
            background: rgba(0, 0, 0, 0.5); color: #fff; padding: 10px;
            font-size: 10px; text-align: left; max-height: 100px; overflow-y: auto;
            pointer-events: auto;
        }

        #instructions {
            padding: 20px; color: white; text-align: center; font-size: 18px; 
            font-weight: bold; text-shadow: 0px 2px 4px black;
            background: linear-gradient(to top, rgba(0,0,0,0.8), transparent);
        }
    </style>
</head>
<body>

    <div id="ui-layer">
        <div id="status-bar">Initializing System...</div>
        <div>
            <div id="instructions">SCANNING... TAP TO PLACE</div>
            <div id="debug-log">Log started...</div>
        </div>
    </div>

    <video id="videoInput" playsinline webkit-playsinline></video>
    <canvas id="canvasOutput"></canvas>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js" crossorigin="anonymous"></script>

    <script>
        function log(msg) {
            const logDiv = document.getElementById('debug-log');
            logDiv.innerHTML += `<div>> ${msg}</div>`;
            logDiv.scrollTop = logDiv.scrollHeight;
            console.log(msg);
        }

        function updateStatus(msg) {
            document.getElementById('status-bar').innerText = msg;
        }

        var Module = {
            onRuntimeInitialized: function() {
                updateStatus("OpenCV Compiled.");
                log("OpenCV WASM Ready.");
                startCamera();
            }
        };
    </script>

    <script async src="https://docs.opencv.org/4.8.0/opencv.js" type="text/javascript"></script>

    <script>
        let video, canvas;
        let renderer, scene, camera, cube, featurePointsMesh;
        let isTracking = false;
        let isScanning = false;
        let streamWidth, streamHeight;

        let cap, oldFrame, oldGray, frame, gray;
        let p0, p1, st, err;
        let initialObjectPoints = [];
        let rvec, tvec, distCoeffs, cameraMatrix;
        
        // Smoothing Variables
        const SMOOTHING_FACTOR = 0.1; // Smoother movement
        let targetPosition = new THREE.Vector3();
        let targetQuaternion = new THREE.Quaternion();

        // PnP Variables
        let imagePointsMat, objectPointsMat;

        async function startCamera() {
            video = document.getElementById('videoInput');
            updateStatus("Requesting Camera...");

            const constraints = {
                video: {
                    facingMode: 'environment',
                    width: { ideal: 640 },
                    height: { ideal: 480 }
                },
                audio: false
            };

            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.muted = true;
                
                const playPromise = video.play();
                if (playPromise !== undefined) {
                    playPromise.catch(error => {
                        updateStatus("TAP SCREEN TO ENABLE CAMERA");
                        document.body.addEventListener('click', () => video.play(), { once: true });
                    });
                }

                const checkVideo = setInterval(() => {
                    if (video.videoWidth > 0 && video.videoHeight > 0) {
                        clearInterval(checkVideo);
                        streamWidth = video.videoWidth;
                        streamHeight = video.videoHeight;
                        video.width = streamWidth;
                        video.height = streamHeight;
                        log(`Video Ready: ${streamWidth}x${streamHeight}`);
                        initAR();
                    }
                }, 100);

            } catch (err) {
                updateStatus("Camera Error");
                log("Error: " + err.message);
            }
        }

        function initAR() {
            try {
                log("Initializing AR Engine...");
                
                canvas = document.getElementById('canvasOutput');
                canvas.width = window.innerWidth;
                canvas.height = window.innerHeight;
                
                initThreeJS();

                cap = new cv.VideoCapture(video);
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                
                p0 = new cv.Mat();
                p1 = new cv.Mat();
                st = new cv.Mat();
                err = new cv.Mat();

                const focalLength = streamWidth; 
                const centerX = streamWidth / 2;
                const centerY = streamHeight / 2;
                
                cameraMatrix = cv.matFromArray(3, 3, cv.CV_64FC1, [
                    focalLength, 0, centerX,
                    0, focalLength, centerY,
                    0, 0, 1
                ]);
                
                distCoeffs = cv.Mat.zeros(4, 1, cv.CV_64FC1);
                rvec = new cv.Mat(); // Initial Rotation (0,0,0)
                tvec = new cv.Mat(); // Initial Translation

                updateStatus("SCANNING SURFACES...");
                log("Scan Mode Active.");
                
                isScanning = true;
                document.body.addEventListener('click', handleUserClick);
                requestAnimationFrame(processLoop);

            } catch (e) {
                log("Init Crashed: " + e.message);
            }
        }

        function initThreeJS() {
            scene = new THREE.Scene();
            // Create a camera that matches video aspect ratio but renders to full screen
            const fov = 2 * Math.atan((streamHeight / 2) / streamWidth) * (180 / Math.PI);
            camera = new THREE.PerspectiveCamera(fov, window.innerWidth / window.innerHeight, 0.1, 1000);
            
            renderer = new THREE.WebGLRenderer({ canvas: canvas, alpha: true, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            scene.add(new THREE.AmbientLight(0xffffff, 1));
            const dl = new THREE.DirectionalLight(0xffffff, 0.8);
            dl.position.set(0, 1, 1);
            scene.add(dl);

            // Large Axis Helper for Orientation Debugging
            scene.add(new THREE.AxesHelper(2));

            const geometry = new THREE.BoxGeometry(0.5, 0.5, 0.5);
            const material = new THREE.MeshNormalMaterial({ wireframe: false }); 
            cube = new THREE.Mesh(geometry, material);
            cube.visible = false; 
            scene.add(cube);

            // Create Point Cloud for Scanning Visualization
            const pointsGeo = new THREE.BufferGeometry();
            const positions = new Float32Array(150 * 3); 
            pointsGeo.setAttribute('position', new THREE.BufferAttribute(positions, 3));
            const pointsMat = new THREE.PointsMaterial({ color: 0xffff00, size: 6, sizeAttenuation: false });
            featurePointsMesh = new THREE.Points(pointsGeo, pointsMat);
            featurePointsMesh.frustumCulled = false;
            scene.add(featurePointsMesh);
        }

        function handleUserClick(e) {
            if (isTracking) {
                isTracking = false;
                isScanning = true;
                cube.visible = false;
                featurePointsMesh.visible = true;
                updateStatus("SCANNING... Tap to Place");
                return;
            }

            if (isScanning && p0.rows > 8) {
                startTracking();
            } else {
                updateStatus("Surface not detected. Move camera.");
            }
        }

        function checkDimensions() {
            if (video.videoWidth !== streamWidth || video.videoHeight !== streamHeight) {
                streamWidth = video.videoWidth;
                streamHeight = video.videoHeight;
                video.width = streamWidth;
                video.height = streamHeight;
                
                frame.delete(); gray.delete(); oldFrame.delete(); oldGray.delete();
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                return true; 
            }
            return false;
        }

        function updateScanVisuals(pointMat) {
            const positions = featurePointsMesh.geometry.attributes.position.array;
            let ptr = 0;
            const scale = 0.01;
            
            for (let i = 0; i < pointMat.rows; i++) {
                let x = pointMat.data32F[i * 2];
                let y = pointMat.data32F[i * 2 + 1];
                
                // Visualization Only: Map 2D -> 3D Screen Space
                positions[ptr++] = (x - streamWidth/2) * scale;
                positions[ptr++] = -(y - streamHeight/2) * scale;
                positions[ptr++] = -5; // Fixed depth 5 meters in front
            }
            
            for (let i = pointMat.rows; i < 150; i++) {
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
            }
            
            featurePointsMesh.geometry.attributes.position.needsUpdate = true;
            
            // Reset camera during scan so overlay matches video
            camera.position.set(0,0,0);
            camera.quaternion.set(0,0,0,1);
        }

        function startTracking() {
            log("Locking features...");
            
            // Lock the current 'p0' points as the World Anchor (Z=0)
            initialObjectPoints = [];
            for (let i = 0; i < p0.rows; i++) {
                let x = p0.data32F[i * 2];
                let y = p0.data32F[i * 2 + 1];
                // Scale factor 0.01 means 100 pixels = 1 meter approx
                initialObjectPoints.push((x - streamWidth/2) * 0.01);
                initialObjectPoints.push(-(y - streamHeight/2) * 0.01); 
                initialObjectPoints.push(0); 
            }

            isTracking = true;
            cube.visible = true;
            featurePointsMesh.visible = false;
            
            // Place cube exactly at World Center (0,0,0)
            // This ensures it spawns in the middle of the tracked features
            cube.position.set(0, 0, 0); 
            
            // Reset Target for Smoothing
            targetPosition.set(0,0,5); // Initial guess back a bit
            targetQuaternion.set(0,0,0,1);
            
            updateStatus("LOCKED! Move Camera.");
        }

        function processLoop() {
            if (!video || !video.videoWidth) {
                requestAnimationFrame(processLoop);
                return;
            }

            try {
                cap.read(frame);
                cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

                if (isScanning) {
                    let none = new cv.Mat();
                    cv.goodFeaturesToTrack(gray, p0, 100, 0.15, 7, none, 7, false, 0.04);
                    updateScanVisuals(p0);
                    gray.copyTo(oldGray); // Update reference constantly in scan
                    none.delete();
                    
                } else if (isTracking) {
                    // Track features from Anchor Frame (oldGray) to Current Frame (gray)
                    cv.calcOpticalFlowPyrLK(oldGray, gray, p0, p1, st, err, new cv.Size(31, 31), 2);

                    let goodNew = [];
                    let goodOld3D = [];
                    let validCount = 0;
                    
                    for (let i = 0; i < st.rows; i++) {
                        if (st.data[i] === 1) {
                            goodNew.push(p1.data32F[i*2]);
                            goodNew.push(p1.data32F[i*2+1]);
                            
                            goodOld3D.push(initialObjectPoints[i*3]);
                            goodOld3D.push(initialObjectPoints[i*3+1]);
                            goodOld3D.push(initialObjectPoints[i*3+2]);
                            validCount++;
                        }
                    }

                    if (validCount >= 6) { 
                        if(imagePointsMat) imagePointsMat.delete();
                        if(objectPointsMat) objectPointsMat.delete();
                        
                        imagePointsMat = cv.matFromArray(validCount, 2, cv.CV_64FC1, goodNew);
                        objectPointsMat = cv.matFromArray(validCount, 3, cv.CV_64FC1, goodOld3D);
                        
                        // FIX: Use Extrinsic Guess to prevent Rotation Flips
                        // We use the rvec/tvec from the previous frame as a hint
                        const success = cv.solvePnP(
                            objectPointsMat, 
                            imagePointsMat, 
                            cameraMatrix, 
                            distCoeffs, 
                            rvec, 
                            tvec, 
                            true, // useExtrinsicGuess = TRUE (Stabilizes rotation)
                            cv.SOLVEPNP_ITERATIVE
                        );
                        
                        if (success) updateThreeJSCamera(rvec, tvec);
                        
                    } else {
                        isTracking = false;
                        isScanning = true;
                        cube.visible = false;
                        featurePointsMesh.visible = true;
                        updateStatus("Lost Track. Scanning...");
                    }
                }

            } catch(e) {
                log("Loop Error: " + e.message);
                isTracking = false;
                isScanning = true;
            }
            
            // --- SMOOTHING STEP ---
            if(isTracking) {
                camera.position.lerp(targetPosition, SMOOTHING_FACTOR);
                camera.quaternion.slerp(targetQuaternion, SMOOTHING_FACTOR);
            }

            renderer.render(scene, camera);
            requestAnimationFrame(processLoop);
        }

        function updateThreeJSCamera(rvec, tvec) {
            try {
                const R = new cv.Mat();
                cv.Rodrigues(rvec, R);

                const t = [tvec.data64F[0], tvec.data64F[1], tvec.data64F[2]];
                const r = R.data64F;

                const m = new THREE.Matrix4();
                m.set(
                    r[0], r[1], r[2], t[0],
                    r[3], r[4], r[5], t[1],
                    r[6], r[7], r[8], t[2],
                    0, 0, 0, 1
                );

                // Invert because PnP gives World->Camera, we need Camera->World
                const inverse = m.invert();
                
                targetQuaternion.setFromRotationMatrix(inverse);
                targetPosition.setFromMatrixPosition(inverse);
                
                // Fix OpenCV (Look +Z) to OpenGL (Look -Z) Coordinate System
                const rotX = new THREE.Quaternion().setFromAxisAngle(new THREE.Vector3(1,0,0), Math.PI);
                targetQuaternion.multiply(rotX);

                R.delete();
            } catch(e) { }
        }
    </script>
</body>
</html>