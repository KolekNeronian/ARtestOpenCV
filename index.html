<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>Custom OpenCV.js AR Engine</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: monospace;}
        
        #videoInput { 
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            object-fit: cover; z-index: 1;
        }
        canvas { 
            display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; 
            z-index: 2; pointer-events: none; 
        }

        #ui-layer {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 10;
            pointer-events: none; display: flex; flex-direction: column; justify-content: space-between;
        }
        
        #status-bar {
            background: rgba(0, 0, 0, 0.7); color: #00ff00; padding: 15px;
            text-align: center; font-size: 16px; border-bottom: 1px solid #00ff00;
        }

        #debug-log {
            background: rgba(0, 0, 0, 0.5); color: #fff; padding: 10px;
            font-size: 10px; text-align: left; max-height: 100px; overflow-y: auto;
            pointer-events: auto;
        }

        #instructions {
            padding: 20px; color: white; text-align: center; font-size: 18px; 
            font-weight: bold; text-shadow: 0px 2px 4px black;
            background: linear-gradient(to top, rgba(0,0,0,0.8), transparent);
        }
    </style>
</head>
<body>

    <div id="ui-layer">
        <div id="status-bar">Initializing System...</div>
        <div>
            <div id="instructions">SCANNING... TAP ANYWHERE TO PLACE</div>
            <div id="debug-log">Log started...</div>
        </div>
    </div>

    <video id="videoInput" playsinline webkit-playsinline></video>
    <canvas id="canvasOutput"></canvas>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js" crossorigin="anonymous"></script>

    <script>
        function log(msg) {
            const logDiv = document.getElementById('debug-log');
            logDiv.innerHTML += `<div>> ${msg}</div>`;
            logDiv.scrollTop = logDiv.scrollHeight;
            console.log(msg);
        }

        function updateStatus(msg) {
            document.getElementById('status-bar').innerText = msg;
        }

        var Module = {
            onRuntimeInitialized: function() {
                updateStatus("OpenCV Compiled.");
                log("OpenCV WASM Ready.");
                startCamera();
            }
        };
    </script>

    <script async src="https://docs.opencv.org/4.8.0/opencv.js" type="text/javascript"></script>

    <script>
        let video, canvas;
        let renderer, scene, camera, cube, featurePointsMesh;
        let isTracking = false;
        let isScanning = false;
        let streamWidth, streamHeight;

        let cap, oldFrame, oldGray, frame, gray;
        let p0, p1, st, err;
        let initialObjectPoints = [];
        let rvec, tvec, distCoeffs, cameraMatrix;
        let useFallbackMath = false;

        // Smoothing Variables
        const SMOOTHING_FACTOR = 0.15; // Lower = Smoother but laggier. Higher = Jittery.
        let targetPosition = new THREE.Vector3();
        let targetQuaternion = new THREE.Quaternion();

        // PnP Variables
        let imagePointsMat, objectPointsMat;

        async function startCamera() {
            video = document.getElementById('videoInput');
            updateStatus("Requesting Camera...");

            const constraints = {
                video: {
                    facingMode: 'environment',
                    width: { ideal: 640 },
                    height: { ideal: 480 }
                },
                audio: false
            };

            try {
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.muted = true;
                
                const playPromise = video.play();
                if (playPromise !== undefined) {
                    playPromise.catch(error => {
                        updateStatus("TAP SCREEN TO ENABLE CAMERA");
                        document.body.addEventListener('click', () => video.play(), { once: true });
                    });
                }

                const checkVideo = setInterval(() => {
                    if (video.videoWidth > 0 && video.videoHeight > 0) {
                        clearInterval(checkVideo);
                        streamWidth = video.videoWidth;
                        streamHeight = video.videoHeight;
                        video.width = streamWidth;
                        video.height = streamHeight;
                        log(`Video Ready: ${streamWidth}x${streamHeight}`);
                        initAR();
                    }
                }, 100);

            } catch (err) {
                updateStatus("Camera Error");
                log("Error: " + err.message);
            }
        }

        function initAR() {
            try {
                log("Initializing AR Engine...");
                
                if (typeof cv.solvePnP !== 'function') {
                    log("WARNING: cv.solvePnP missing. Using Homography fallback.");
                    useFallbackMath = true;
                }

                canvas = document.getElementById('canvasOutput');
                canvas.width = window.innerWidth;
                canvas.height = window.innerHeight;
                
                initThreeJS();

                cap = new cv.VideoCapture(video);
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                
                p0 = new cv.Mat();
                p1 = new cv.Mat();
                st = new cv.Mat();
                err = new cv.Mat();

                const focalLength = streamWidth; 
                const centerX = streamWidth / 2;
                const centerY = streamHeight / 2;
                
                cameraMatrix = cv.matFromArray(3, 3, cv.CV_64FC1, [
                    focalLength, 0, centerX,
                    0, focalLength, centerY,
                    0, 0, 1
                ]);
                
                distCoeffs = cv.Mat.zeros(4, 1, cv.CV_64FC1);
                rvec = new cv.Mat();
                tvec = new cv.Mat();

                updateStatus("SCANNING SURFACES...");
                log("Scan Mode Active.");
                
                isScanning = true;
                document.body.addEventListener('click', handleUserClick);
                requestAnimationFrame(processLoop);

            } catch (e) {
                log("Init Crashed: " + e.message);
            }
        }

        function initThreeJS() {
            scene = new THREE.Scene();
            // Create a camera that matches video aspect ratio but renders to full screen
            const fov = 2 * Math.atan((streamHeight / 2) / streamWidth) * (180 / Math.PI);
            camera = new THREE.PerspectiveCamera(fov, window.innerWidth / window.innerHeight, 0.1, 1000);
            
            renderer = new THREE.WebGLRenderer({ canvas: canvas, alpha: true, antialias: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            scene.add(new THREE.AmbientLight(0xffffff, 1));
            const dl = new THREE.DirectionalLight(0xffffff, 0.8);
            dl.position.set(0, 1, 1);
            scene.add(dl);

            scene.add(new THREE.AxesHelper(1));

            const geometry = new THREE.BoxGeometry(0.5, 0.5, 0.5);
            // Changed to NormalMaterial for better visibility
            const material = new THREE.MeshNormalMaterial({ wireframe: false }); 
            cube = new THREE.Mesh(geometry, material);
            cube.visible = false; 
            scene.add(cube);

            // Create Point Cloud for Scanning Visualization
            const pointsGeo = new THREE.BufferGeometry();
            const positions = new Float32Array(150 * 3); 
            pointsGeo.setAttribute('position', new THREE.BufferAttribute(positions, 3));
            const pointsMat = new THREE.PointsMaterial({ color: 0xffff00, size: 4, sizeAttenuation: false });
            featurePointsMesh = new THREE.Points(pointsGeo, pointsMat);
            featurePointsMesh.frustumCulled = false;
            scene.add(featurePointsMesh);
        }

        function handleUserClick(e) {
            if (isTracking) {
                // Reset to scanning
                isTracking = false;
                isScanning = true;
                cube.visible = false;
                featurePointsMesh.visible = true;
                updateStatus("SCANNING... Tap to Place");
                return;
            }

            if (isScanning && p0.rows > 8) {
                // Pass the click event so we know WHERE to place the cube
                startTracking(e.clientX, e.clientY);
            } else {
                updateStatus("Surface not detected. Move camera.");
            }
        }

        function checkDimensions() {
            if (video.videoWidth !== streamWidth || video.videoHeight !== streamHeight) {
                streamWidth = video.videoWidth;
                streamHeight = video.videoHeight;
                video.width = streamWidth;
                video.height = streamHeight;
                
                frame.delete(); gray.delete(); oldFrame.delete(); oldGray.delete();
                frame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                gray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                oldFrame = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC4);
                oldGray = new cv.Mat(streamHeight, streamWidth, cv.CV_8UC1);
                return true; 
            }
            return false;
        }

        function updateScanVisuals(pointMat) {
            const positions = featurePointsMesh.geometry.attributes.position.array;
            let ptr = 0;
            const scale = 0.01;
            
            for (let i = 0; i < pointMat.rows; i++) {
                let x = pointMat.data32F[i * 2];
                let y = pointMat.data32F[i * 2 + 1];
                
                positions[ptr++] = (x - streamWidth/2) * scale;
                positions[ptr++] = -(y - streamHeight/2) * scale;
                positions[ptr++] = -5; 
            }
            
            for (let i = pointMat.rows; i < 150; i++) {
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
                positions[ptr++] = 99999;
            }
            
            featurePointsMesh.geometry.attributes.position.needsUpdate = true;
            camera.position.set(0,0,0);
            camera.rotation.set(0,0,0);
        }

        function startTracking(clickX, clickY) {
            log("Locking features...");
            
            initialObjectPoints = [];
            for (let i = 0; i < p0.rows; i++) {
                let x = p0.data32F[i * 2];
                let y = p0.data32F[i * 2 + 1];
                // Map to 3D Plane Z=0
                initialObjectPoints.push((x - streamWidth/2) * 0.01);
                initialObjectPoints.push(-(y - streamHeight/2) * 0.01); 
                initialObjectPoints.push(0); 
            }

            isTracking = true;
            cube.visible = true;
            featurePointsMesh.visible = false;
            
            // --- FIX PLACEMENT LOGIC ---
            // Calculate where the user clicked relative to the center of the stream
            // We use the same scale (0.01) as we used for the world points
            
            // Adjust for canvas stretching if CSS resizes it
            const rect = canvas.getBoundingClientRect();
            const scaleX = canvas.width / rect.width;
            const scaleY = canvas.height / rect.height;
            
            // Get click position in Canvas Coordinates
            const canvasX = (clickX - rect.left) * scaleX;
            const canvasY = (clickY - rect.top) * scaleY;

            // Since video aspect might differ from screen aspect, we map relative to video center
            // NOTE: This assumes video fills screen or is centered. 
            // For MVP, we treat screen pixels == video pixels roughly
            
            // Map to World Coordinates (same logic as initialObjectPoints)
            const worldClickX = (canvasX - streamWidth/2) * 0.01;
            const worldClickY = -(canvasY - streamHeight/2) * 0.01;

            cube.position.set(worldClickX, worldClickY, 0);
            
            // Reset Target for Smoothing to prevent initial jump
            targetPosition.set(0,0,0);
            targetQuaternion.set(0,0,0,1);
            camera.position.set(0,0,0);
            camera.quaternion.set(0,0,0,1);

            updateStatus("LOCKED! Move Camera.");
        }

        function processLoop() {
            if (!video || !video.videoWidth) {
                requestAnimationFrame(processLoop);
                return;
            }

            try {
                cap.read(frame);
                cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

                if (isScanning) {
                    let none = new cv.Mat();
                    // Quality 0.15 for better points
                    cv.goodFeaturesToTrack(gray, p0, 100, 0.15, 7, none, 7, false, 0.04);
                    updateScanVisuals(p0);
                    gray.copyTo(oldGray);
                    none.delete();
                    
                } else if (isTracking) {
                    // Larger Search Window (31x31) reduces jitter
                    cv.calcOpticalFlowPyrLK(oldGray, gray, p0, p1, st, err, new cv.Size(31, 31), 2);

                    let goodNew = [];
                    let goodOld3D = [];
                    let validCount = 0;
                    
                    for (let i = 0; i < st.rows; i++) {
                        if (st.data[i] === 1) {
                            goodNew.push(p1.data32F[i*2]);
                            goodNew.push(p1.data32F[i*2+1]);
                            
                            goodOld3D.push(initialObjectPoints[i*3]);
                            goodOld3D.push(initialObjectPoints[i*3+1]);
                            goodOld3D.push(initialObjectPoints[i*3+2]);
                            validCount++;
                        }
                    }

                    if (validCount >= 6) { // Require more points for stability
                        if(imagePointsMat) imagePointsMat.delete();
                        if(objectPointsMat) objectPointsMat.delete();
                        
                        imagePointsMat = cv.matFromArray(validCount, 2, cv.CV_64FC1, goodNew);
                        
                        if (useFallbackMath) {
                            solveHomographyPose(goodOld3D, goodNew, rvec, tvec);
                        } else {
                            objectPointsMat = cv.matFromArray(validCount, 3, cv.CV_64FC1, goodOld3D);
                            const success = cv.solvePnP(objectPointsMat, imagePointsMat, cameraMatrix, distCoeffs, rvec, tvec, false, cv.SOLVEPNP_ITERATIVE);
                            if (success) updateThreeJSCamera(rvec, tvec);
                        }
                    } else {
                        isTracking = false;
                        isScanning = true;
                        cube.visible = false;
                        featurePointsMesh.visible = true;
                        updateStatus("Lost Track. Scanning...");
                    }
                }

            } catch(e) {
                log("Loop Error: " + e.message);
                isTracking = false;
                isScanning = true;
            }
            
            // --- SMOOTHING STEP ---
            if(isTracking) {
                camera.position.lerp(targetPosition, SMOOTHING_FACTOR);
                camera.quaternion.slerp(targetQuaternion, SMOOTHING_FACTOR);
            }

            renderer.render(scene, camera);
            requestAnimationFrame(processLoop);
        }

        function solveHomographyPose(objPoints, imgPoints, rvec, tvec) {
            let cx_new = 0, cy_new = 0;
            let cx_old = 0, cy_old = 0;
            const count = imgPoints.length / 2;
            
            for(let i=0; i<count; i++) {
                cx_new += imgPoints[i*2];
                cy_new += imgPoints[i*2+1];
                cx_old += (objPoints[i*3] / 0.01) + streamWidth/2;
                cy_old += -(objPoints[i*3+1] / 0.01) + streamHeight/2;
            }
            
            const dx = ((cx_new/count) - (cx_old/count));
            const dy = ((cy_new/count) - (cy_old/count));
            
            tvec.data64F[0] = dx * 0.01;
            tvec.data64F[1] = -dy * 0.01;
            tvec.data64F[2] = 0;
            
            rvec.data64F[0] = 0; rvec.data64F[1] = 0; rvec.data64F[2] = 0;
            updateThreeJSCamera(rvec, tvec);
        }

        function updateThreeJSCamera(rvec, tvec) {
            try {
                const R = new cv.Mat();
                cv.Rodrigues(rvec, R);

                const t = [tvec.data64F[0], tvec.data64F[1], tvec.data64F[2]];
                const r = R.data64F;

                const m = new THREE.Matrix4();
                m.set(
                    r[0], r[1], r[2], t[0],
                    r[3], r[4], r[5], t[1],
                    r[6], r[7], r[8], t[2],
                    0, 0, 0, 1
                );

                const inverse = m.invert();
                
                // Instead of setting camera directly, set TARGET for smoothing
                targetQuaternion.setFromRotationMatrix(inverse);
                targetPosition.setFromMatrixPosition(inverse);
                
                // Handle coordinate system flip in the target 
                // (Create temporary object to apply rotation then extract)
                // Actually easier to just apply the rotation to the QUATERNION
                const rotX = new THREE.Quaternion().setFromAxisAngle(new THREE.Vector3(1,0,0), Math.PI);
                targetQuaternion.multiply(rotX);

                R.delete();
            } catch(e) { }
        }
    </script>
</body>
</html>